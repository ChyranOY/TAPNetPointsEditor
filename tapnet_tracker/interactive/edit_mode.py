#!/usr/bin/env python3
"""
Interactive editing mode for TAPNet Tracker.

Contains functions for interactive track editing, point manipulation, and coordinate updates.
"""

import os
import cv2
import torch
import tempfile
import numpy as np
from typing import Tuple, List, Dict, Optional, Any
from PIL import Image

from ..utils.visualization import generate_point_colors
from ..utils.track_utils import unzip_to_array
# ç»Ÿä¸€ä½¿ç”¨configä¸­çš„è¾“å‡ºç›®å½•é…ç½®


class TrackEditManager:
    """è½¨è¿¹ç¼–è¾‘ç®¡ç†å™¨ç±»"""
    
    def __init__(self):
        self.tracks_data = None
        self.video_path = None
        self.estimated_original_size = None
        self.original_raw_data = None
        self.original_tracks_shape = None
        self.frame_count = 0
        self.selected_point = None
        self.current_frame_index = 0
        self.point_info = []
        self.point_colors = None  # ğŸ¨ å›ºå®šçš„ç‚¹é¢œè‰²æ˜ å°„ï¼Œç¡®ä¿ç¼–è¾‘ç•Œé¢å’Œå¯è§†åŒ–ä¸€è‡´
        
    def initialize_edit_mode(self, video_path: str, tracks_path: str) -> Tuple[bool, str, Optional[Tuple]]:
        """
        è¿›å…¥ç¼–è¾‘æ¨¡å¼ï¼ŒåŠ è½½è§†é¢‘å’Œè½¨è¿¹æ–‡ä»¶
        
        Args:
            video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
            tracks_path: è½¨è¿¹æ–‡ä»¶è·¯å¾„
            
        Returns:
            Tuple[success, message, initial_frame_data]
        """
        try:
            # éªŒè¯æ–‡ä»¶å­˜åœ¨
            if not os.path.exists(video_path):
                return False, f"âŒ è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_path}", None
            
            if not os.path.exists(tracks_path):
                return False, f"âŒ è½¨è¿¹æ–‡ä»¶ä¸å­˜åœ¨: {tracks_path}", None
            
            # è·å–è§†é¢‘ä¿¡æ¯
            cap = cv2.VideoCapture(video_path)
            if not cap.isOpened():
                return False, f"âŒ æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶: {video_path}", None
            
            width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
            height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
            frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            cap.release()
            
            print(f"ğŸ¬ è§†é¢‘ä¿¡æ¯: {width}x{height}, {frame_count} å¸§")
            
            # ä½¿ç”¨å½“å‰è§†é¢‘å°ºå¯¸ä½œä¸ºåŸå§‹å°ºå¯¸
            original_size = (width, height)
            print(f"ğŸ“ è½¨è¿¹ç¼–è¾‘æ¨¡å¼ï¼šä½¿ç”¨å½“å‰è§†é¢‘å°ºå¯¸ä½œä¸ºåŸå§‹å°ºå¯¸: {original_size}")
            
            # åŠ è½½è½¨è¿¹æ•°æ®
            print("ğŸ“Š åŠ è½½è½¨è¿¹æ–‡ä»¶...")
            if isinstance(tracks_path, str):
                tracks_loaded = torch.load(tracks_path)
            else:
                tracks_loaded = tracks_path
            
            tracks_np = unzip_to_array(tracks_loaded)
            
            # è®°å½•åŸå§‹æ•°æ®ä¿¡æ¯
            self.original_tracks_shape = tracks_np.shape
            self.original_raw_data = tracks_np.copy()
            
            print(f"ğŸ” åŸå§‹è½¨è¿¹æ•°æ®å½¢çŠ¶: {tracks_np.shape}")
            print(f"ğŸ” åŸå§‹è½¨è¿¹æ•°æ®èŒƒå›´: min={tracks_np.min():.2f}, max={tracks_np.max():.2f}")
            
            # å¤„ç†è½¨è¿¹æ•°æ®ç”¨äºç¼–è¾‘
            tracks_processed = self.process_tracks_for_editing(
                tracks_np, (width, height), original_size, quant_multi=8
            )
            
            # è®¾ç½®å®ä¾‹å˜é‡
            self.tracks_data = tracks_processed
            self.video_path = video_path
            self.estimated_original_size = original_size
            self.frame_count = frame_count
            self.selected_point = None
            self.current_frame_index = 0
            self.point_info = []
            
            print(f"âœ… ç¼–è¾‘æ¨¡å¼åˆå§‹åŒ–æˆåŠŸ!")
            print(f"  - è§†é¢‘: {os.path.basename(video_path)}")
            print(f"  - è½¨è¿¹: {os.path.basename(tracks_path)}")
            print(f"  - è½¨è¿¹æ•°æ®å½¢çŠ¶: {tracks_processed.shape}")
            print(f"  - è§†é¢‘å°ºå¯¸: {original_size}")
            
            # ç”Ÿæˆç¬¬ä¸€å¸§
            frame_pil, error, point_info = self.extract_frame_with_tracks(
                video_path, tracks_processed, 0, original_size
            )
            
            if frame_pil is not None:
                self.point_info = point_info
                
                return True, "âœ… ç¼–è¾‘æ¨¡å¼åˆå§‹åŒ–æˆåŠŸ", (frame_pil, None, [])
            else:
                return False, f"âŒ æ— æ³•ç”Ÿæˆç¬¬ä¸€å¸§: {error}", None
                
        except Exception as e:
            import traceback
            error_detail = traceback.format_exc()
            return False, f"âŒ åˆå§‹åŒ–ç¼–è¾‘æ¨¡å¼å¤±è´¥: {str(e)}\nè¯¦ç»†é”™è¯¯:\n{error_detail}", None
    
    def process_tracks_for_editing(self, tracks_np: np.ndarray, target_frame_size: Tuple[int, int], 
                                 original_size: Tuple[int, int], quant_multi: int = 8):
        """
        å¤„ç†è½¨è¿¹æ•°æ®ä»¥ç”¨äºç¼–è¾‘æ¨¡å¼
        
        Args:
            tracks_np: åŸå§‹è½¨è¿¹æ•°æ®
            target_frame_size: ç›®æ ‡å¸§å°ºå¯¸
            original_size: åŸå§‹å°ºå¯¸
            quant_multi: é‡åŒ–å€æ•°
            
        Returns:
            å¤„ç†åçš„è½¨è¿¹æ•°æ®ï¼Œé€‚åˆç¼–è¾‘ä½¿ç”¨
        """
        print(f"ğŸ”§ å¤„ç†è½¨è¿¹æ•°æ®ç”¨äºç¼–è¾‘æ¨¡å¼:")
        print(f"  - è¾“å…¥æ•°æ®å½¢çŠ¶: {tracks_np.shape}")
        print(f"  - ç›®æ ‡å¸§å°ºå¯¸: {target_frame_size}")
        print(f"  - åŸå§‹å°ºå¯¸: {original_size}")
        print(f"  - é‡åŒ–å€æ•°: {quant_multi}")
        
        # åé‡åŒ–ï¼ˆé™¤ä»¥8ï¼‰
        tracks_dequantized = tracks_np / quant_multi
        print(f"  - åé‡åŒ–åæ•°æ®èŒƒå›´: min={tracks_dequantized.min():.2f}, max={tracks_dequantized.max():.2f}")
        
        # ç¡®å®šæ•°æ®æ ¼å¼å¹¶è½¬æ¢ä¸º [frames, points, 3] æ ¼å¼
        if len(tracks_dequantized.shape) == 4:
            if tracks_dequantized.shape[0] < tracks_dequantized.shape[1]:
                # [points, frames, batch, 3] -> [frames, points, 3]
                print(f"  - è½¬æ¢æ ¼å¼: [points, frames, batch, 3] -> [frames, points, 3]")
                tracks_reshaped = tracks_dequantized.transpose(1, 0, 2, 3)  # [frames, points, batch, 3]
                tracks_reshaped = tracks_reshaped.squeeze(2)  # [frames, points, 3]
            else:
                # [frames, channels, points, 3] -> [frames, points, 3]
                print(f"  - è½¬æ¢æ ¼å¼: [frames, channels, points, 3] -> [frames, points, 3]")
                tracks_reshaped = tracks_dequantized[:, 0, :, :]  # å–ç¬¬ä¸€ä¸ªchannel
        else:
            # [frames, points, 3] æ ¼å¼
            print(f"  - ä¿æŒæ ¼å¼: [frames, points, 3]")
            tracks_reshaped = tracks_dequantized
        
        # åªä½¿ç”¨3é€šé“æ•°æ® [x, y, visibility]
        tracks_with_time = tracks_reshaped
        
        print(f"  - æœ€ç»ˆæ•°æ®å½¢çŠ¶: {tracks_with_time.shape}")
        print(f"  - åæ ‡èŒƒå›´: X=[{tracks_with_time[:, :, 0].min():.1f}, {tracks_with_time[:, :, 0].max():.1f}]")
        print(f"  - åæ ‡èŒƒå›´: Y=[{tracks_with_time[:, :, 1].min():.1f}, {tracks_with_time[:, :, 1].max():.1f}]")
        print(f"  - å¯è§æ€§èŒƒå›´: [{tracks_with_time[:, :, 2].min():.1f}, {tracks_with_time[:, :, 2].max():.1f}]")
        
        return torch.from_numpy(tracks_with_time).float()
    
    def extract_frame_with_tracks(self, video_path: str, tracks_data: torch.Tensor, 
                                frame_index: int, estimated_original_size: Tuple[int, int], 
                                selected_point: int = None, highlight_point: int = None):
        """
        æå–æŒ‡å®šå¸§å¹¶æ·»åŠ è½¨è¿¹å¯è§†åŒ–ï¼ˆäº¤äº’å¼ç‰ˆæœ¬ï¼‰
        
        Args:
            video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
            tracks_data: è½¨è¿¹æ•°æ®
            frame_index: å¸§ç´¢å¼•
            estimated_original_size: åŸå§‹å°ºå¯¸
            selected_point: é€‰ä¸­çš„ç‚¹ID
            highlight_point: é«˜äº®çš„ç‚¹ID
            
        Returns:
            Tuple[frame, error_message, point_info]
        """
        try:
            # æ‰“å¼€è§†é¢‘
            cap = cv2.VideoCapture(video_path)
            
            # è·å–è§†é¢‘å±æ€§
            width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
            height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
            frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            
            # è·³è½¬åˆ°æŒ‡å®šå¸§
            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_index)
            ret, frame = cap.read()
            cap.release()
            
            if not ret:
                return None, f"æ— æ³•è¯»å–å¸§ {frame_index}", []
            
            # åˆ†ç¦»è½¨è¿¹æ•°æ® - [x, y, visibility]æ ¼å¼
            tracks, visible = torch.split(tracks_data, [2, 1], dim=-1)
            
            # ç›´æ¥ä½¿ç”¨å¸§ç´¢å¼•ï¼ˆ1:1å¯¹åº”ï¼‰
            track_frame_idx = min(frame_index, len(tracks) - 1)
            
            # è·å–å½“å‰å¸§çš„è½¨è¿¹ç‚¹
            current_tracks = tracks[track_frame_idx]  # [points, 2]
            current_visible = visible[track_frame_idx]  # [points, 1]
            
            # ä»256x256ç©ºé—´æ˜ å°„åˆ°åŸå§‹è§†é¢‘å°ºå¯¸
            scale_x = width / 256.0
            scale_y = height / 256.0
            pixel_coords = current_tracks * torch.tensor([scale_x, scale_y]).type_as(current_tracks)
            
            print(f"ğŸ”§ åæ ‡æ˜ å°„è°ƒè¯• - å¸§ {frame_index}:")
            print(f"   è§†é¢‘å°ºå¯¸: {width}x{height}")
            print(f"   ç¼©æ”¾å› å­: scale_x={scale_x:.3f}, scale_y={scale_y:.3f}")
            print(f"   256ç©ºé—´åæ ‡èŒƒå›´: X=[{current_tracks[:, 0].min():.1f}, {current_tracks[:, 0].max():.1f}], Y=[{current_tracks[:, 1].min():.1f}, {current_tracks[:, 1].max():.1f}]")
            print(f"   æ˜ å°„ååæ ‡èŒƒå›´: X=[{pixel_coords[:, 0].min():.1f}, {pixel_coords[:, 0].max():.1f}], Y=[{pixel_coords[:, 1].min():.1f}, {pixel_coords[:, 1].max():.1f}]")
            
            # å‡†å¤‡ç‚¹ä¿¡æ¯ï¼ˆç”¨äºç‚¹å‡»æ£€æµ‹ï¼‰
            point_info = []
            
            # ğŸ¨ ä½¿ç”¨å›ºå®šçš„é¢œè‰²æ˜ å°„ï¼Œç¡®ä¿ä¸å¯è§†åŒ–è§†é¢‘ä¸€è‡´
            num_points = len(pixel_coords)
            if self.point_colors is None or len(self.point_colors) != num_points:
                # ç¬¬ä¸€æ¬¡åˆå§‹åŒ–æˆ–ç‚¹æ•°é‡å˜åŒ–æ—¶é‡æ–°ç”Ÿæˆé¢œè‰²
                self.point_colors = generate_point_colors(num_points)
                print(f"ğŸ¨ ç”Ÿæˆå›ºå®šé¢œè‰²æ˜ å°„: {num_points}ä¸ªç‚¹")
            point_colors = self.point_colors
            
            # ç»˜åˆ¶è½¨è¿¹ç‚¹
            for i in range(num_points):
                x, y = pixel_coords[i].tolist()
                is_visible = current_visible[i, 0] > 0
                
                # æ£€æŸ¥ç‚¹æ˜¯å¦åœ¨å›¾åƒè¾¹ç•Œå†…
                in_bounds = 0 <= x < width and 0 <= y < height
                
                # è®°å½•ç‚¹ä¿¡æ¯
                point_info.append({
                    'id': i,
                    'x': x,
                    'y': y,
                    'visible': is_visible,
                    'in_bounds': in_bounds
                })
                
                # åªç»˜åˆ¶åœ¨è¾¹ç•Œå†…çš„ç‚¹
                if in_bounds:
                    # æ ¹æ®é€‰ä¸­çŠ¶æ€ç¡®å®šç»˜åˆ¶æ ·å¼
                    if selected_point is not None and i == selected_point:
                        # é€‰ä¸­çš„ç‚¹
                        color = point_colors[i] if is_visible else (128, 128, 128)
                        radius = 8
                        border_radius = 10
                        border_thickness = 3
                        border_color = (255, 255, 255)
                    elif highlight_point is not None and i == highlight_point:
                        # é«˜äº®ç‚¹
                        color = point_colors[i] if is_visible else (160, 160, 160)
                        radius = 6
                        border_radius = 8
                        border_thickness = 2
                        border_color = (255, 255, 255)
                    else:
                        # æ™®é€šç‚¹
                        if is_visible:
                            color = point_colors[i]
                            radius = 4
                            border_radius = 6
                            border_thickness = 1
                            border_color = (255, 255, 255)
                        else:
                            color = (200, 200, 200)
                            radius = 3
                            border_radius = 5
                            border_thickness = 1
                            border_color = (180, 180, 180)
                    
                    # ç»˜åˆ¶è¾¹æ¡†å’Œå†…åœ†
                    cv2.circle(frame, (int(x), int(y)), border_radius, border_color, border_thickness)
                    cv2.circle(frame, (int(x), int(y)), radius, color, -1)
                    
                    # ç»˜åˆ¶ç‚¹ID
                    if selected_point == i or highlight_point == i:
                        text_color = (255, 255, 255)
                    elif is_visible:
                        text_color = (0, 0, 0)
                    else:
                        text_color = (128, 128, 128)
                    
                    cv2.putText(frame, str(i), (int(x) + 12, int(y) - 8), 
                               cv2.FONT_HERSHEY_SIMPLEX, 0.4, text_color, 1)
            
            # è½¬æ¢å›¾åƒæ ¼å¼ï¼šBGR -> RGB -> PIL
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            frame_pil = Image.fromarray(frame_rgb)
            
            return frame_pil, None, point_info
            
        except Exception as e:
            return None, f"Error extracting frame: {e}", []
    
    def find_nearest_point(self, click_x: float, click_y: float, max_distance: float = 20) -> Optional[int]:
        """
        æŸ¥æ‰¾æœ€è¿‘çš„ç‚¹
        
        Args:
            click_x: ç‚¹å‡»çš„xåæ ‡
            click_y: ç‚¹å‡»çš„yåæ ‡
            max_distance: æœ€å¤§è·ç¦»é˜ˆå€¼
            
        Returns:
            æœ€è¿‘ç‚¹çš„IDï¼Œå¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆ™è¿”å›None
        """
        min_distance = float('inf')
        nearest_point = None
        
        for point in self.point_info:
            if point['visible'] and point['in_bounds']:
                distance = ((click_x - point['x']) ** 2 + (click_y - point['y']) ** 2) ** 0.5
                if distance < max_distance and distance < min_distance:
                    min_distance = distance
                    nearest_point = point['id']
        
        return nearest_point
    
    def update_point_coordinates(self, point_id: int, new_x: float, new_y: float, frame_index: int) -> Tuple[bool, str]:
        """
        æ›´æ–°ç‚¹çš„åæ ‡
        
        Args:
            point_id: ç‚¹ID
            new_x: æ–°çš„xåæ ‡
            new_y: æ–°çš„yåæ ‡
            frame_index: å¸§ç´¢å¼•
            
        Returns:
            Tuple[success, message]
        """
        try:
            if self.tracks_data is None:
                return False, "è¯·å…ˆå¤„ç†è§†é¢‘å’Œè½¨è¿¹æ–‡ä»¶"
            
            # è·å–è§†é¢‘å°ºå¯¸
            cap = cv2.VideoCapture(self.video_path)
            width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
            height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
            cap.release()
            
            # åˆ†ç¦»è½¨è¿¹æ•°æ® - [x, y, visibility]æ ¼å¼
            tracks, visible = torch.split(self.tracks_data, [2, 1], dim=-1)
            
            # ç›´æ¥ä½¿ç”¨å¸§ç´¢å¼•
            track_frame_idx = min(frame_index, len(tracks) - 1)
            
            # æ£€æŸ¥ç´¢å¼•æ˜¯å¦åœ¨æœ‰æ•ˆèŒƒå›´å†…
            if point_id >= tracks.shape[1]:
                return False, f"ç‚¹ID {point_id} è¶…å‡ºèŒƒå›´ï¼Œæœ€å¤§ç‚¹æ•°ä¸º {tracks.shape[1]}"
            
            # ä»åŸå§‹è§†é¢‘å°ºå¯¸æ˜ å°„å›256x256ç©ºé—´
            scale_x = 256.0 / width
            scale_y = 256.0 / height
            new_coords_256 = torch.tensor([new_x * scale_x, new_y * scale_y]).type_as(tracks)
            
            print(f"ğŸ”§ åæ ‡æ›´æ–°è°ƒè¯•:")
            print(f"   åŸå§‹è§†é¢‘å°ºå¯¸: {width}x{height}")
            print(f"   UIç‚¹å‡»åæ ‡: ({new_x:.1f}, {new_y:.1f})")
            print(f"   æ˜ å°„åˆ°256ç©ºé—´: ({new_coords_256[0]:.1f}, {new_coords_256[1]:.1f})")
            
            # æ›´æ–°è½¨è¿¹æ•°æ®ï¼ˆä¿å­˜256ç©ºé—´çš„åæ ‡ï¼‰
            tracks[track_frame_idx, point_id, 0] = new_coords_256[0]
            tracks[track_frame_idx, point_id, 1] = new_coords_256[1]
            
            # é‡æ–°ç»„åˆè½¨è¿¹æ•°æ®
            self.tracks_data = torch.cat([tracks, visible], dim=-1)
            
            # æ›´æ–°åŸå§‹æ•°æ®ï¼ˆä¼ é€’256ç©ºé—´çš„åæ ‡ï¼‰
            self._update_original_data_coordinates_256(point_id, new_coords_256[0].item(), new_coords_256[1].item(), track_frame_idx)
            
            return True, f"å·²æ›´æ–°ç‚¹ {point_id} çš„åæ ‡åˆ° ({new_x:.0f}, {new_y:.0f})"
            
        except Exception as e:
            return False, f"æ›´æ–°åæ ‡æ—¶å‡ºé”™: {str(e)}"
    
    def toggle_point_visibility(self, point_id: int, frame_index: int) -> Tuple[bool, str]:
        """
        åˆ‡æ¢ç‚¹çš„å¯è§æ€§
        
        Args:
            point_id: ç‚¹ID
            frame_index: å¸§ç´¢å¼•
            
        Returns:
            Tuple[success, message]
        """
        try:
            if self.tracks_data is None:
                return False, "è¯·å…ˆå¤„ç†è§†é¢‘å’Œè½¨è¿¹æ–‡ä»¶"
            
            # åˆ†ç¦»è½¨è¿¹æ•°æ® - [x, y, visibility]æ ¼å¼
            tracks, visible = torch.split(self.tracks_data, [2, 1], dim=-1)
            
            # ç›´æ¥ä½¿ç”¨å¸§ç´¢å¼•
            track_frame_idx = min(frame_index, len(tracks) - 1)
            
            # æ£€æŸ¥ç´¢å¼•æ˜¯å¦åœ¨æœ‰æ•ˆèŒƒå›´å†…
            if point_id >= tracks.shape[1]:
                return False, f"ç‚¹ID {point_id} è¶…å‡ºèŒƒå›´ï¼Œæœ€å¤§ç‚¹æ•°ä¸º {tracks.shape[1]}"
            
            # è·å–å½“å‰å¯è§æ€§çŠ¶æ€å¹¶åˆ‡æ¢
            current_visibility = visible[track_frame_idx, point_id, 0]
            new_visibility = 1.0 - current_visibility  # åˆ‡æ¢ï¼š0->1, 1->0
            
            # æ›´æ–°å¯è§æ€§
            visible[track_frame_idx, point_id, 0] = new_visibility
            
            # é‡æ–°ç»„åˆè½¨è¿¹æ•°æ®
            self.tracks_data = torch.cat([tracks, visible], dim=-1)
            
            # æ›´æ–°åŸå§‹æ•°æ®
            self._update_original_data_visibility(point_id, new_visibility, track_frame_idx)
            
            visibility_text = "å¯è§" if new_visibility > 0.1 else "ä¸å¯è§"
            return True, f"å·²å°†ç‚¹ {point_id} çš„å¯è§æ€§åˆ‡æ¢ä¸º: {visibility_text}"
            
        except Exception as e:
            return False, f"åˆ‡æ¢å¯è§æ€§æ—¶å‡ºé”™: {str(e)}"
    
    def _update_original_data_coordinates_256(self, point_id: int, coord_256_x: float, coord_256_y: float, 
                                            track_frame_idx: int):
        """æ›´æ–°åŸå§‹æ•°æ®ä¸­çš„åæ ‡ï¼ˆè¾“å…¥å·²ç»æ˜¯256ç©ºé—´åæ ‡ï¼‰"""
        if self.original_raw_data is None:
            return
        
        try:
            # ç›´æ¥é‡åŒ–256ç©ºé—´çš„åæ ‡
            orig_x_quantized = coord_256_x * 8
            orig_y_quantized = coord_256_y * 8
            
            print(f"ğŸ”§ åŸå§‹æ•°æ®æ›´æ–°è°ƒè¯•:")
            print(f"   256ç©ºé—´åæ ‡: ({coord_256_x:.1f}, {coord_256_y:.1f})")
            print(f"   é‡åŒ–ååæ ‡: ({orig_x_quantized:.1f}, {orig_y_quantized:.1f})")
            
            # æ ¹æ®æ•°æ®æ ¼å¼æ›´æ–°: [points, frames, batch, 3] - (x, y, visibility)
            if len(self.original_raw_data.shape) == 4:
                if self.original_raw_data.shape[0] < self.original_raw_data.shape[1]:
                    # [points, frames, batch, 3] æ ¼å¼ - (x, y, visibility)
                    self.original_raw_data[point_id, track_frame_idx, 0, 0] = orig_x_quantized  # xåæ ‡åœ¨ç¬¬1ä½
                    self.original_raw_data[point_id, track_frame_idx, 0, 1] = orig_y_quantized  # yåæ ‡åœ¨ç¬¬2ä½
                else:
                    # [frames, channels, points, 3] æ ¼å¼
                    self.original_raw_data[track_frame_idx, :, point_id, 0] = orig_x_quantized
                    self.original_raw_data[track_frame_idx, :, point_id, 1] = orig_y_quantized
            else:
                # [frames, points, 3] æ ¼å¼
                self.original_raw_data[track_frame_idx, point_id, 0] = orig_x_quantized
                self.original_raw_data[track_frame_idx, point_id, 1] = orig_y_quantized
                
        except Exception as e:
            print(f"âŒ åŸå§‹æ•°æ®åæ ‡æ›´æ–°å¤±è´¥: {str(e)}")
    
    def _update_original_data_visibility(self, point_id: int, new_visibility: float, track_frame_idx: int):
        """æ›´æ–°åŸå§‹æ•°æ®ä¸­çš„å¯è§æ€§"""
        if self.original_raw_data is None:
            return
        
        try:
            # é‡åŒ–å¯è§æ€§å€¼
            orig_visibility_quantized = new_visibility * 8
            
            # æ ¹æ®æ•°æ®æ ¼å¼æ›´æ–°: [points, frames, batch, 3] - (x, y, visibility)
            if len(self.original_raw_data.shape) == 4:
                if self.original_raw_data.shape[0] < self.original_raw_data.shape[1]:
                    # [points, frames, batch, 3] æ ¼å¼ - (x, y, visibility)
                    self.original_raw_data[point_id, track_frame_idx, 0, 2] = orig_visibility_quantized  # visibilityåœ¨ç¬¬3ä½
                else:
                    # [frames, channels, points, 3] æ ¼å¼
                    self.original_raw_data[track_frame_idx, :, point_id, 2] = orig_visibility_quantized
            else:
                # [frames, points, 3] æ ¼å¼
                self.original_raw_data[track_frame_idx, point_id, 2] = orig_visibility_quantized
                
        except Exception as e:
            print(f"âŒ åŸå§‹æ•°æ®å¯è§æ€§æ›´æ–°å¤±è´¥: {str(e)}")
    
    def save_modified_tracks(self, save_path: str = None) -> Tuple[Optional[str], str]:
        """
        ä¿å­˜ä¿®æ”¹åçš„è½¨è¿¹æ•°æ®
        
        Args:
            save_path: ä¿å­˜è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨ç”Ÿæˆ
            
        Returns:
            Tuple[file_path, message]
        """
        try:
            if self.original_raw_data is None:
                return None, "âŒ æ²¡æœ‰å¯ä¿å­˜çš„è½¨è¿¹æ•°æ®"
            
            if save_path is None:
                # è‡ªåŠ¨ç”Ÿæˆä¿å­˜è·¯å¾„ - ä¿å­˜åˆ°è§†é¢‘ä¸“ç”¨æ–‡ä»¶å¤¹
                video_name = os.path.splitext(os.path.basename(self.video_path))[0]
                from ..core.config import config
                base_output_dir = config.ensure_output_dir()
                video_output_dir = os.path.join(base_output_dir, video_name)
                os.makedirs(video_output_dir, exist_ok=True)
                save_path = os.path.join(video_output_dir, f"{video_name}.pth")
            
            # å‹ç¼©å¹¶ä¿å­˜æ•°æ®
            with tempfile.NamedTemporaryFile(suffix='.npz', delete=False) as npz_file:
                np.savez_compressed(npz_file, array=self.original_raw_data)
                temp_npz_path = npz_file.name
            
            # è¯»å–å‹ç¼©æ•°æ®
            with open(temp_npz_path, 'rb') as f:
                compressed_data = f.read()
            
            # ä¿å­˜ä¸º.pthæ–‡ä»¶
            torch.save(compressed_data, save_path)
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            os.unlink(temp_npz_path)
            
            print(f"âœ… ç¼–è¾‘åçš„è½¨è¿¹å·²ä¿å­˜åˆ°: {save_path}")
            
            success_msg = f"""âœ… è½¨è¿¹ä¿å­˜æˆåŠŸ!

ğŸ“ **æ–‡ä»¶è·¯å¾„**: `{save_path}`
ğŸ“Š **æ•°æ®å½¢çŠ¶**: {self.original_raw_data.shape}
ğŸ¬ **åŸå§‹è§†é¢‘**: {os.path.basename(self.video_path)}

ğŸ’¡ **æç¤º**: ä¿å­˜çš„æ˜¯ç¼–è¾‘åçš„åŸå§‹æ ¼å¼æ•°æ®ï¼Œå¯ç”¨äºé‡æ–°åŠ è½½ç¼–è¾‘æˆ–ç”Ÿæˆæ–°çš„å¯è§†åŒ–è§†é¢‘ã€‚
"""
            
            return save_path, success_msg
            
        except Exception as e:
            import traceback
            error_detail = traceback.format_exc()
            return None, f"âŒ ä¿å­˜è½¨è¿¹å¤±è´¥: {str(e)}\nè¯¦ç»†é”™è¯¯:\n{error_detail}"


# å…¨å±€å®ä¾‹ï¼ˆç”¨äºå‘åå…¼å®¹ï¼‰
_global_edit_manager = TrackEditManager()


# å‘åå…¼å®¹çš„å‡½æ•°æ¥å£
def extract_frame_with_tracks_interactive(video_path, tracks_data, frame_index, estimated_original_size, 
                                        selected_point=None, highlight_point=None):
    """æå–æŒ‡å®šå¸§å¹¶æ·»åŠ è½¨è¿¹å¯è§†åŒ–ï¼ˆå‘åå…¼å®¹å‡½æ•°ï¼‰"""
    return _global_edit_manager.extract_frame_with_tracks(
        video_path, tracks_data, frame_index, estimated_original_size, selected_point, highlight_point
    )


def find_nearest_point(click_x, click_y, point_info, max_distance=20):
    """æŸ¥æ‰¾æœ€è¿‘çš„ç‚¹ï¼ˆå‘åå…¼å®¹å‡½æ•°ï¼‰"""
    # æ›´æ–°å…¨å±€ç®¡ç†å™¨çš„ç‚¹ä¿¡æ¯
    _global_edit_manager.point_info = point_info
    return _global_edit_manager.find_nearest_point(click_x, click_y, max_distance)


def update_point_coordinates(point_id, new_x, new_y, frame_index):
    """æ›´æ–°ç‚¹çš„åæ ‡ï¼ˆå‘åå…¼å®¹å‡½æ•°ï¼‰"""
    return _global_edit_manager.update_point_coordinates(point_id, new_x, new_y, frame_index)


def toggle_point_visibility(point_id, frame_index):
    """åˆ‡æ¢ç‚¹çš„å¯è§æ€§ï¼ˆå‘åå…¼å®¹å‡½æ•°ï¼‰"""
    return _global_edit_manager.toggle_point_visibility(point_id, frame_index)


def save_modified_tracks(save_path=None):
    """ä¿å­˜ä¿®æ”¹åçš„è½¨è¿¹æ•°æ®ï¼ˆå‘åå…¼å®¹å‡½æ•°ï¼‰"""
    return _global_edit_manager.save_modified_tracks(save_path) 